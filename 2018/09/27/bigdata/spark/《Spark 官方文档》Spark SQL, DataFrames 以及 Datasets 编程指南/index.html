<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/bugs/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/bugs/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/bugs/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/bugs/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/bugs/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/bugs/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/bugs/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/bugs/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/bugs/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="大数据,spark," />










<meta name="description" content="本文由 简悦 SimpRead 转码， 原文地址 http://ifeve.com/spark-sql-dataframes/ spark-1.6.0 [原文地址]  Spark SQL, DataFrames 以及 Datasets 编程指南概要Spark SQL 是 Spark 中处理结构化数据的模块。与基础的 Spark RDD API 不同，Spark SQL 的接口提供了更多关于数据的">
<meta name="keywords" content="大数据,spark">
<meta property="og:type" content="article">
<meta property="og:title" content="《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南">
<meta property="og:url" content="https://deanwang1943.github.io/bugs/2018/09/27/bigdata/spark/《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南/index.html">
<meta property="og:site_name" content="Bugs">
<meta property="og:description" content="本文由 简悦 SimpRead 转码， 原文地址 http://ifeve.com/spark-sql-dataframes/ spark-1.6.0 [原文地址]  Spark SQL, DataFrames 以及 Datasets 编程指南概要Spark SQL 是 Spark 中处理结构化数据的模块。与基础的 Spark RDD API 不同，Spark SQL 的接口提供了更多关于数据的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-09-27T14:03:52.516Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南">
<meta name="twitter:description" content="本文由 简悦 SimpRead 转码， 原文地址 http://ifeve.com/spark-sql-dataframes/ spark-1.6.0 [原文地址]  Spark SQL, DataFrames 以及 Datasets 编程指南概要Spark SQL 是 Spark 中处理结构化数据的模块。与基础的 Spark RDD API 不同，Spark SQL 的接口提供了更多关于数据的">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/bugs/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":true,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://deanwang1943.github.io/bugs/2018/09/27/bigdata/spark/《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南/"/>





  <title>《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南 | Bugs</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/bugs/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Bugs</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">微笑的周末</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/bugs/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="https://github.com/deanwang1943/blog/blob/master/blog/%E7%AE%80%E5%8E%86.md" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/bugs/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/bugs/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/bugs/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-github">
          <a href="https://github.com/deanwang1943" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-github"></i> <br />
            
            github
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://deanwang1943.github.io/bugs/bugs/2018/09/27/bigdata/spark/《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Dean Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/bugs/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Bugs">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">《Spark 官方文档》Spark SQL, DataFrames 以及 Datasets 编程指南</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-09-27T00:36:03+00:00">
                2018-09-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/bugs/categories/大数据/" itemprop="url" rel="index">
                    <span itemprop="name">大数据</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  11.7k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  48 分钟
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>本文由 <a href="http://ksria.com/simpread/" target="_blank" rel="noopener">简悦 SimpRead</a> 转码， 原文地址 <a href="http://ifeve.com/spark-sql-dataframes/" target="_blank" rel="noopener">http://ifeve.com/spark-sql-dataframes/</a> spark-1.6.0 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html" target="_blank" rel="noopener">[原文地址]</a></p>
</blockquote>
<h2 id="Spark-SQL-DataFrames-以及-Datasets-编程指南"><a href="#Spark-SQL-DataFrames-以及-Datasets-编程指南" class="headerlink" title="Spark SQL, DataFrames 以及 Datasets 编程指南"></a>Spark SQL, DataFrames 以及 Datasets 编程指南</h2><h1 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h1><p>Spark SQL 是 Spark 中处理结构化数据的模块。与基础的 Spark RDD API 不同，Spark SQL 的接口提供了更多关于数据的结构信息和计算任务的运行时信息。在 Spark 内部，Spark SQL 会能够用于做优化的信息比 RDD API 更多一些。Spark SQL 如今有了三种不同的 API：SQL 语句、DataFrame API 和最新的 Dataset API。不过真正运行计算的时候，无论你使用哪种 API 或语言，Spark SQL 使用的执行引擎都是同一个。这种底层的统一，使开发者可以在不同的 API 之间来回切换，你可以选择一种最自然的方式，来表达你的需求。</p>
<p>本文中所有的示例都使用 Spark 发布版本中自带的示例数据，并且可以在 spark-shell、pyspark shell 以及 sparkR shell 中运行。</p>
<h2 id="SQL"><a href="#SQL" class="headerlink" title="SQL"></a>SQL</h2><p>Spark SQL 的一种用法是直接执行 SQL 查询语句，你可使用最基本的 SQL 语法，也可以选择 HiveQL 语法。Spark SQL 可以从已有的 Hive 中读取数据。更详细的请参考 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables" target="_blank" rel="noopener">Hive Tables</a> 这一节。如果用其他编程语言运行 SQL，Spark SQL 将以 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#DataFrames" target="_blank" rel="noopener">DataFrame</a> 返回结果。你还可以通过命令行 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-spark-sql-cli" target="_blank" rel="noopener">command-line</a> 或者 <a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#running-the-thrift-jdbcodbc-server" target="_blank" rel="noopener">JDBC/ODBC</a> 使用 Spark SQL。</p>
<h2 id="DataFrames"><a href="#DataFrames" class="headerlink" title="DataFrames"></a>DataFrames</h2><p>DataFrame 是一种分布式数据集合，每一条数据都由几个命名字段组成。概念上来说，她和关系型数据库的表 或者 R 和 Python 中的 data frame 等价，只不过在底层，DataFrame 采用了更多优化。DataFrame 可以从很多数据源（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">sources</a>）加载数据并构造得到，如：结构化数据文件，Hive 中的表，外部数据库，或者已有的 RDD。</p>
<p>DataFrame API 支持 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a>, and <a href="http://spark.apache.org/docs/latest/api/R/index.html" target="_blank" rel="noopener">R</a>。</p>
<h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p>Dataset 是 Spark-1.6 新增的一种 API，目前还是实验性的。Dataset 想要把 RDD 的优势（强类型，可以使用 lambda 表达式函数）和 Spark SQL 的优化执行引擎的优势结合到一起。Dataset 可以由 JVM 对象构建（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#creating-datasets" target="_blank" rel="noopener">constructed</a> ）得到，而后 Dataset 上可以使用各种 transformation 算子（map，flatMap，filter 等）。</p>
<p>Dataset API 对 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset" target="_blank" rel="noopener">Scala</a> 和 <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/Dataset.html" target="_blank" rel="noopener">Java</a> 的支持接口是一致的，但目前还不支持 Python，不过 Python 自身就有语言动态特性优势（例如，你可以使用字段名来访问数据，row.columnName）。对 Python 的完整支持在未来的版本会增加进来。</p>
<h1 id="入门"><a href="#入门" class="headerlink" title="入门"></a>入门</h1><h2 id="入口：SQLContext"><a href="#入口：SQLContext" class="headerlink" title="入口：SQLContext"></a>入口：SQLContext</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_0" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_0" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_0" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><strong><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_0" target="_blank" rel="noopener">R</a></strong></li>
</ul>
<p>Spark SQL 所有的功能入口都是 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext" target="_blank" rel="noopener"><code>SQLContext</code></a> 类，及其子类。不过要创建一个 SQLContext 对象，首先需要有一个 SparkContext 对象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">val sc: SparkContext // 假设已经有一个 SparkContext 对象</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">// 用于包含RDD到DataFrame隐式转换操作</span><br><span class="line">import sqlContext.implicits._</span><br></pre></td></tr></table></figure>
<p>除了 SQLContext 之外，你也可以创建 HiveContext，HiveContext 是 SQLContext 的超集。</p>
<p>除了 SQLContext 的功能之外，HiveContext 还提供了完整的 HiveQL 语法，UDF 使用，以及对 Hive 表中数据的访问。要使用 HiveContext，你并不需要安装 Hive，而且 SQLContext 能用的数据源，HiveContext 也一样能用。HiveContext 是单独打包的，从而避免了在默认的 Spark 发布版本中包含所有的 Hive 依赖。如果这些依赖对你来说不是问题（不会造成依赖冲突等），建议你在 Spark-1.3 之前使用 HiveContext。而后续的 Spark 版本，将会逐渐把 SQLContext 升级到和 HiveContext 功能差不多的状态。</p>
<p>spark.sql.dialect 选项可以指定不同的 SQL 变种（或者叫 SQL 方言）。这个参数可以在 SparkContext.setConf 里指定，也可以通过 SQL 语句的 SET key=value 命令指定。对于 SQLContext，该配置目前唯一的可选值就是”sql”，这个变种使用一个 Spark SQL 自带的简易 SQL 解析器。而对于 HiveContext，spark.sql.dialect 默认值为”hiveql”，当然你也可以将其值设回”sql”。仅就目前而言，HiveSQL 解析器支持更加完整的 SQL 语法，所以大部分情况下，推荐使用 HiveContext。</p>
<h2 id="创建-DataFrame"><a href="#创建-DataFrame" class="headerlink" title="创建 DataFrame"></a>创建 DataFrame</h2><p>Spark 应用可以用 SparkContext 创建 DataFrame，所需的数据来源可以是已有的 RDD（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interoperating-with-rdds" target="_blank" rel="noopener">existing <code>RDD</code></a>），或者 Hive 表，或者其他数据源（<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#data-sources" target="_blank" rel="noopener">data sources</a>.）</p>
<p>以下是一个从 JSON 文件创建 DataFrame 的小栗子：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_1" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_1" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_1" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_1" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">val sc: SparkContext // 已有的 SparkContext.</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">val df = sqlContext.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// 将DataFrame内容打印到stdout</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure>
<h2 id="DataFrame-操作"><a href="#DataFrame-操作" class="headerlink" title="DataFrame 操作"></a>DataFrame 操作</h2><p>DataFrame 提供了结构化数据的领域专用语言支持，包括 <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">Scala</a>, <a href="http://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/sql/DataFrame.html" target="_blank" rel="noopener">Java</a>, <a href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame" target="_blank" rel="noopener">Python</a> and <a href="http://spark.apache.org/docs/latest/api/R/DataFrame.html" target="_blank" rel="noopener">R</a>.</p>
<p>这里我们给出一个结构化数据处理的基本示例：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_2" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_2" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_2" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_2" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">val sc: SparkContext // 已有的 SparkContext.</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">// 创建一个 DataFrame</span><br><span class="line">val df = sqlContext.read.json(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line"></span><br><span class="line">// 展示 DataFrame 的内容</span><br><span class="line">df.show()</span><br><span class="line">// age  name</span><br><span class="line">// null Michael</span><br><span class="line">// 30   Andy</span><br><span class="line">// 19   Justin</span><br><span class="line"></span><br><span class="line">// 打印数据树形结构</span><br><span class="line">df.printSchema()</span><br><span class="line">// root</span><br><span class="line">// |-- age: long (nullable = true)</span><br><span class="line">// |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// select &quot;name&quot; 字段</span><br><span class="line">df.select(&quot;name&quot;).show()</span><br><span class="line">// name</span><br><span class="line">// Michael</span><br><span class="line">// Andy</span><br><span class="line">// Justin</span><br><span class="line"></span><br><span class="line">// 展示所有人，但所有人的 age 都加1</span><br><span class="line">df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show()</span><br><span class="line">// name    (age + 1)</span><br><span class="line">// Michael null</span><br><span class="line">// Andy    31</span><br><span class="line">// Justin  20</span><br><span class="line"></span><br><span class="line">// 筛选出年龄大于21的人</span><br><span class="line">df.filter(df(&quot;age&quot;) &gt; 21).show()</span><br><span class="line">// age name</span><br><span class="line">// 30  Andy</span><br><span class="line"></span><br><span class="line">// 计算各个年龄的人数</span><br><span class="line">df.groupBy(&quot;age&quot;).count().show()</span><br><span class="line">// age  count</span><br><span class="line">// null 1</span><br><span class="line">// 19   1</span><br><span class="line">// 30   1</span><br></pre></td></tr></table></figure>
<p>DataFrame 的完整 API 列表请参考这里：<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame" target="_blank" rel="noopener">API Documentation</a></p>
<p>除了简单的字段引用和表达式支持之外，DataFrame 还提供了丰富的工具函数库，包括字符串组装，日期处理，常见的数学函数等。完整列表见这里：<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$" target="_blank" rel="noopener">DataFrame Function Reference</a>.</p>
<h2 id="编程方式执行-SQL-查询"><a href="#编程方式执行-SQL-查询" class="headerlink" title="编程方式执行 SQL 查询"></a>编程方式执行 SQL 查询</h2><p>SQLContext.sql 可以执行一个 SQL 查询，并返回 DataFrame 结果。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_3" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_3" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_3" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_3" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val sqlContext = ... // 已有一个 SQLContext 对象</span><br><span class="line">val df = sqlContext.sql(&quot;SELECT * FROM table&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="创建-Dataset"><a href="#创建-Dataset" class="headerlink" title="创建 Dataset"></a>创建 Dataset</h2><p>Dataset API 和 RDD 类似，不过 Dataset 不使用 Java 序列化或者 Kryo，而是使用专用的编码器（<a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder" target="_blank" rel="noopener">Encoder</a> ）来序列化对象和跨网络传输通信。如果这个编码器和标准序列化都能把对象转字节，那么编码器就可以根据代码动态生成，并使用一种特殊数据格式，这种格式下的对象不需要反序列化回来，就能允许 Spark 进行操作，如过滤、排序、哈希等。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_4" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_4" target="_blank" rel="noopener"><strong>Java</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 对普通类型数据的Encoder是由 importing sqlContext.implicits._ 自动提供的</span><br><span class="line">val ds = Seq(1, 2, 3).toDS()</span><br><span class="line">ds.map(_ + 1).collect() // 返回: Array(2, 3, 4)</span><br><span class="line"></span><br><span class="line">// 以下这行不仅定义了case class，同时也自动为其创建了Encoder</span><br><span class="line">case class Person(name: String, age: Long)</span><br><span class="line">val ds = Seq(Person(&quot;Andy&quot;, 32)).toDS()</span><br><span class="line"></span><br><span class="line">// DataFrame 只需提供一个和数据schema对应的class即可转换为 Dataset。Spark会根据字段名进行映射。</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val people = sqlContext.read.json(path).as[Person]</span><br></pre></td></tr></table></figure>
<h2 id="和-RDD-互操作"><a href="#和-RDD-互操作" class="headerlink" title="和 RDD 互操作"></a>和 RDD 互操作</h2><p>Spark SQL 有两种方法将 RDD 转为 DataFrame。</p>
<p>1. 使用反射机制，推导包含指定类型对象 RDD 的 schema。这种基于反射机制的方法使代码更简洁，而且如果你事先知道数据 schema，推荐使用这种方式；</p>
<p>2. 编程方式构建一个 schema，然后应用到指定 RDD 上。这种方式更啰嗦，但如果你事先不知道数据有哪些字段，或者数据 schema 是运行时读取进来的，那么你很可能需要用这种方式。</p>
<h3 id="利用反射推导-schema"><a href="#利用反射推导-schema" class="headerlink" title="利用反射推导 schema"></a>利用反射推导 schema</h3><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_5" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_5" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_5" target="_blank" rel="noopener"><strong>Python</strong></a></li>
</ul>
<p>Spark SQL 的 Scala 接口支持自动将包含 case class 对象的 RDD 转为 DataFrame。对应的 case class 定义了表的 schema。case class 的参数名通过反射，映射为表的字段名。case class 还可以嵌套一些复杂类型，如 Seq 和 Array。RDD 隐式转换成 DataFrame 后，可以进一步注册成表。随后，你就可以对表中数据使用 SQL 语句查询了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">// sc 是已有的 SparkContext 对象</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line">// 为了支持RDD到DataFrame的隐式转换</span><br><span class="line">import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">// 定义一个case class.</span><br><span class="line">// 注意：Scala 2.10的case class最多支持22个字段，要绕过这一限制，</span><br><span class="line">// 你可以使用自定义class，并实现Product接口。当然，你也可以改用编程方式定义schema</span><br><span class="line">case class Person(name: String, age: Int)</span><br><span class="line"></span><br><span class="line">// 创建一个包含Person对象的RDD，并将其注册成table</span><br><span class="line">val people = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;).map(_.split(&quot;,&quot;)).map(p =&gt; Person(p(0), p(1).trim.toInt)).toDF()</span><br><span class="line">people.registerTempTable(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// sqlContext.sql方法可以直接执行SQL语句</span><br><span class="line">val teenagers = sqlContext.sql(&quot;SELECT name, age FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;)</span><br><span class="line"></span><br><span class="line">// SQL查询的返回结果是一个DataFrame，且能够支持所有常见的RDD算子</span><br><span class="line">// 查询结果中每行的字段可以按字段索引访问:</span><br><span class="line">teenagers.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">// 或者按字段名访问:</span><br><span class="line">teenagers.map(t =&gt; &quot;Name: &quot; + t.getAs[String](&quot;name&quot;)).collect().foreach(println)</span><br><span class="line"></span><br><span class="line">// row.getValuesMap[T] 会一次性返回多列，并以Map[String, T]为返回结果类型</span><br><span class="line">teenagers.map(_.getValuesMap[Any](List(&quot;name&quot;, &quot;age&quot;))).collect().foreach(println)</span><br><span class="line">// 返回结果: Map(&quot;name&quot; -&gt; &quot;Justin&quot;, &quot;age&quot; -&gt; 19)</span><br></pre></td></tr></table></figure>
<h3 id="编程方式定义-Schema"><a href="#编程方式定义-Schema" class="headerlink" title="编程方式定义 Schema"></a>编程方式定义 Schema</h3><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_6" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_6" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><strong><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_6" target="_blank" rel="noopener">Python</a></strong></li>
</ul>
<p>如果不能事先通过 case class 定义 schema（例如，记录的字段结构是保存在一个字符串，或者其他文本数据集中，需要先解析，又或者字段对不同用户有所不同），那么你可能需要按以下三个步骤，以编程方式的创建一个 DataFrame：</p>
<ol>
<li>从已有的 RDD 创建一个包含 Row 对象的 RDD</li>
<li>用 StructType 创建一个 schema，和步骤 1 中创建的 RDD 的结构相匹配</li>
<li>把得到的 schema 应用于包含 Row 对象的 RDD，调用这个方法来实现这一步：SQLContext.createDataFrame</li>
</ol>
<p>For example:</p>
<p>例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">// sc 是已有的SparkContext对象</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">// 创建一个RDD</span><br><span class="line">val people = sc.textFile(&quot;examples/src/main/resources/people.txt&quot;)</span><br><span class="line"></span><br><span class="line">// 数据的schema被编码与一个字符串中</span><br><span class="line">val schemaString = &quot;name age&quot;</span><br><span class="line"></span><br><span class="line">// Import Row.</span><br><span class="line">import org.apache.spark.sql.Row;</span><br><span class="line"></span><br><span class="line">// Import Spark SQL 各个数据类型</span><br><span class="line">import org.apache.spark.sql.types.&#123;StructType,StructField,StringType&#125;;</span><br><span class="line"></span><br><span class="line">// 基于前面的字符串生成schema</span><br><span class="line">val schema =</span><br><span class="line">  StructType(</span><br><span class="line">    schemaString.split(&quot; &quot;).map(fieldName =&gt; StructField(fieldName, StringType, true)))</span><br><span class="line"></span><br><span class="line">// 将RDD[people]的各个记录转换为Rows，即：得到一个包含Row对象的RDD</span><br><span class="line">val rowRDD = people.map(_.split(&quot;,&quot;)).map(p =&gt; Row(p(0), p(1).trim))</span><br><span class="line"></span><br><span class="line">// 将schema应用到包含Row对象的RDD上，得到一个DataFrame</span><br><span class="line">val peopleDataFrame = sqlContext.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">// 将DataFrame注册为table</span><br><span class="line">peopleDataFrame.registerTempTable(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// 执行SQL语句</span><br><span class="line">val results = sqlContext.sql(&quot;SELECT name FROM people&quot;)</span><br><span class="line"></span><br><span class="line">// SQL查询的结果是DataFrame，且能够支持所有常见的RDD算子</span><br><span class="line">// 并且其字段可以以索引访问，也可以用字段名访问</span><br><span class="line">results.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h1 id="数据源"><a href="#数据源" class="headerlink" title="数据源"></a>数据源</h1><p>Spark SQL 支持基于 DataFrame 操作一系列不同的数据源。DataFrame 既可以当成一个普通 RDD 来操作，也可以将其注册成一个临时表来查询。把 DataFrame 注册为 table 之后，你就可以基于这个 table 执行 SQL 语句了。本节将描述加载和保存数据的一些通用方法，包含了不同的 Spark 数据源，然后深入介绍一下内建数据源可用选项。</p>
<h2 id="通用加载-保存函数"><a href="#通用加载-保存函数" class="headerlink" title="通用加载 / 保存函数"></a>通用加载 / 保存函数</h2><p>在最简单的情况下，所有操作都会以默认类型数据源来加载数据（默认是 Parquet，除非修改了 spark.sql.sources.default 配置）。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_7" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_7" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_7" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_7" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val df = sqlContext.read.load(&quot;examples/src/main/resources/users.parquet&quot;)</span><br><span class="line">df.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;namesAndFavColors.parquet&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>你也可以手动指定数据源，并设置一些额外的选项参数。数据源可由其全名指定（如，org.apache.spark.sql.parquet），而对于内建支持的数据源，可以使用简写名（json, parquet, jdbc）。任意类型数据源创建的 DataFrame 都可以用下面这种语法转成其他类型数据格式。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_8" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_8" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_8" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_8" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val df = sqlContext.read.format(&quot;json&quot;).load(&quot;examples/src/main/resources/people.json&quot;)</span><br><span class="line">df.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;parquet&quot;).save(&quot;namesAndAges.parquet&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="直接对文件使用-SQL"><a href="#直接对文件使用-SQL" class="headerlink" title="直接对文件使用 SQL"></a>直接对文件使用 SQL</h3><p>Spark SQL 还支持直接对文件使用 SQL 查询，不需要用 read 方法把文件加载进来。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_9" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_9" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_9" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_9" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">val df = sqlContext.sql(&quot;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="保存模式"><a href="#保存模式" class="headerlink" title="保存模式"></a>保存模式</h3><p>Save 操作有一个可选参数 SaveMode，用这个参数可以指定如何处理数据已经存在的情况。很重要的一点是，这些保存模式都没有加锁，所以其操作也不是原子性的。另外，如果使用 Overwrite 模式，实际操作是，先删除数据，再写新数据。</p>
<p>| 仅 Scala/Java | 所有支持的语言 | 含义 |<br>| <code>SaveMode.ErrorIfExists</code>(default) | <code>&quot;error&quot;</code>(default) | （默认模式）从 DataFrame 向数据源保存数据时，如果数据已经存在，则抛异常。 |<br>| <code>SaveMode.Append</code> | <code>&quot;append&quot;</code> | 如果数据或表已经存在，则将 DataFrame 的数据追加到已有数据的尾部。 |<br>| <code>SaveMode.Overwrite</code> | <code>&quot;overwrite&quot;</code> | 如果数据或表已经存在，则用 DataFrame 数据覆盖之。 |<br>| <code>SaveMode.Ignore</code> | <code>&quot;ignore&quot;</code> | 如果数据已经存在，那就放弃保存 DataFrame 数据。这和 SQL 里 CREATE TABLE IF NOT EXISTS 有点类似。 |</p>
<h3 id="保存到持久化表"><a href="#保存到持久化表" class="headerlink" title="保存到持久化表"></a>保存到持久化表</h3><p>在使用 HiveContext 的时候，DataFrame 可以用 saveAsTable 方法，将数据保存成持久化的表。与 registerTempTable 不同，saveAsTable 会将 DataFrame 的实际数据内容保存下来，并且在 HiveMetastore 中创建一个游标指针。持久化的表会一直保留，即使 Spark 程序重启也没有影响，只要你连接到同一个 metastore 就可以读取其数据。读取持久化表时，只需要用用表名作为参数，调用 SQLContext.table 方法即可得到对应 DataFrame。</p>
<p>默认情况下，saveAsTable 会创建一个”managed table“，也就是说这个表数据的位置是由 metastore 控制的。同样，如果删除表，其数据也会同步删除。</p>
<h2 id="Parquet-文件"><a href="#Parquet-文件" class="headerlink" title="Parquet 文件"></a>Parquet 文件</h2><p><a href="http://parquet.io/" target="_blank" rel="noopener">Parquet</a> 是一种流行的列式存储格式。Spark SQL 提供对 Parquet 文件的读写支持，而且 Parquet 文件能够自动保存原始数据的 schema。写 Parquet 文件的时候，所有的字段都会自动转成 nullable，以便向后兼容。</p>
<h3 id="编程方式加载数据"><a href="#编程方式加载数据" class="headerlink" title="编程方式加载数据"></a>编程方式加载数据</h3><p>仍然使用上面例子中的数据：</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_10" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_10" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_10" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_10" target="_blank" rel="noopener"><strong>R</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_sql_10" target="_blank" rel="noopener"><strong>Sql</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">// 我们继续沿用之前例子中的sqlContext对象</span><br><span class="line">// 为了支持RDD隐式转成DataFrame</span><br><span class="line">import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">val people: RDD[Person] = ... // 和上面例子中相同，一个包含case class对象的RDD</span><br><span class="line"></span><br><span class="line">// 该RDD将隐式转成DataFrame，然后保存为parquet文件</span><br><span class="line">people.write.parquet(&quot;people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// 读取上面保存的Parquet文件(多个文件 - Parquet保存完其实是很多个文件)。Parquet文件是自描述的，文件中保存了schema信息</span><br><span class="line">// 加载Parquet文件，并返回DataFrame结果</span><br><span class="line">val parquetFile = sqlContext.read.parquet(&quot;people.parquet&quot;)</span><br><span class="line"></span><br><span class="line">// Parquet文件（多个）可以注册为临时表，然后在SQL语句中直接查询</span><br><span class="line">parquetFile.registerTempTable(&quot;parquetFile&quot;)</span><br><span class="line">val teenagers = sqlContext.sql(&quot;SELECT name FROM parquetFile WHERE age &gt;= 13 AND age &lt;= 19&quot;)</span><br><span class="line">teenagers.map(t =&gt; &quot;Name: &quot; + t(0)).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h3 id="分区发现"><a href="#分区发现" class="headerlink" title="分区发现"></a>分区发现</h3><p>像 Hive 这样的系统，一个很常用的优化手段就是表分区。在一个支持分区的表中，数据是保存在不同的目录中的，并且将分区键以编码方式保存在各个分区目录路径中。Parquet 数据源现在也支持自动发现和推导分区信息。例如，我们可以把之前用的人口数据存到一个分区表中，其目录结构如下所示，其中有 2 个额外的字段，gender 和 country，作为分区键：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=US</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=CN</span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=US</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=CN</span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure>
<p>在这个例子中，如果需要读取 Parquet 文件数据，我们只需要把 path/to/table 作为参数传给 SQLContext.read.parquet 或者 SQLContext.read.load。Spark SQL 能够自动的从路径中提取出分区信息，随后返回的 DataFrame 的 schema 如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable = true)</span><br><span class="line">|-- age: long (nullable = true)</span><br><span class="line">|-- gender: string (nullable = true)</span><br><span class="line">|-- country: string (nullable = true)</span><br></pre></td></tr></table></figure>
<p>注意，分区键的数据类型将是自动推导出来的。目前，只支持数值类型和字符串类型数据作为分区键。</p>
<p>有的用户可能不想要自动推导出来的分区键数据类型。这种情况下，你可以通过 spark.sql.sources.partitionColumnTypeInference.enabled （默认是 true）来禁用分区键类型推导。禁用之后，分区键总是被当成字符串类型。</p>
<p>从 Spark-1.6.0 开始，分区发现默认只在指定目录的子目录中进行。以上面的例子来说，如果用户把 path/to/table/gender=male 作为参数传给 SQLContext.read.parquet 或者 SQLContext.read.load，那么 gender 就不会被作为分区键。如果用户想要指定分区发现的基础目录，可以通过 basePath 选项指定。例如，如果把 path/to/table/gender=male 作为数据目录，并且将 basePath 设为 path/to/table，那么 gender 仍然会最为分区键。</p>
<h3 id="Schema-合并"><a href="#Schema-合并" class="headerlink" title="Schema 合并"></a>Schema 合并</h3><p>像 ProtoBuffer、Avro 和 Thrift 一样，Parquet 也支持 schema 演变。用户从一个简单的 schema 开始，逐渐增加所需的新字段。这样的话，用户最终会得到多个 schema 不同但互相兼容的 Parquet 文件。目前，Parquet 数据源已经支持自动检测这种情况，并合并所有文件的 schema。</p>
<p>因为 schema 合并相对代价比较大，并且在多数情况下不是必要的，所以从 Spark-1.5.0 之后，默认是被禁用的。你可以这样启用这一功能：</p>
<ol>
<li>读取 Parquet 文件时，将选项 mergeSchema 设为 true（见下面的示例代码）</li>
<li>或者，将全局选项 spark.sql.parquet.mergeSchema 设为 true</li>
</ol>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_11" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_11" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_11" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// 继续沿用之前的sqlContext对象</span><br><span class="line">// 为了支持RDD隐式转换为DataFrame</span><br><span class="line">import sqlContext.implicits._</span><br><span class="line"></span><br><span class="line">// 创建一个简单的DataFrame，存到一个分区目录中</span><br><span class="line">val df1 = sc.makeRDD(1 to 5).map(i =&gt; (i, i * 2)).toDF(&quot;single&quot;, &quot;double&quot;)</span><br><span class="line">df1.write.parquet(&quot;data/test_table/key=1&quot;)</span><br><span class="line"></span><br><span class="line">// 创建另一个DataFrame放到新的分区目录中，</span><br><span class="line">// 并增加一个新字段，丢弃一个老字段</span><br><span class="line">val df2 = sc.makeRDD(6 to 10).map(i =&gt; (i, i * 3)).toDF(&quot;single&quot;, &quot;triple&quot;)</span><br><span class="line">df2.write.parquet(&quot;data/test_table/key=2&quot;)</span><br><span class="line"></span><br><span class="line">// 读取分区表</span><br><span class="line">val df3 = sqlContext.read.option(&quot;mergeSchema&quot;, &quot;true&quot;).parquet(&quot;data/test_table&quot;)</span><br><span class="line">df3.printSchema()</span><br><span class="line"></span><br><span class="line">// 最终的schema将由3个字段组成（single，double，triple）</span><br><span class="line">// 并且分区键出现在目录路径中</span><br><span class="line">// root</span><br><span class="line">// |-- single: int (nullable = true)</span><br><span class="line">// |-- double: int (nullable = true)</span><br><span class="line">// |-- triple: int (nullable = true)</span><br><span class="line">// |-- key : int (nullable = true)</span><br></pre></td></tr></table></figure>
<h3 id="Hive-metastore-Parquet-table-转换"><a href="#Hive-metastore-Parquet-table-转换" class="headerlink" title="Hive metastore Parquet table 转换"></a>Hive metastore Parquet table 转换</h3><p>在读写 Hive metastore Parquet 表时，Spark SQL 用的是内部的 Parquet 支持库，而不是 Hive SerDe，因为这样性能更好。这一行为是由 spark.sql.hive.convertMetastoreParquet 配置项来控制的，而且默认是启用的。</p>
<h4 id="Hive-Parquet-schema-调和"><a href="#Hive-Parquet-schema-调和" class="headerlink" title="Hive/Parquet schema 调和"></a>Hive/Parquet schema 调和</h4><p>Hive 和 Parquet 在表结构处理上主要有 2 个不同点：</p>
<ol>
<li>Hive 大小写敏感，而 Parquet 不是</li>
<li>Hive 所有字段都是 nullable 的，而 Parquet 需要显示设置</li>
</ol>
<p>由于以上原因，我们必须在 Hive metastore Parquet table 转 Spark SQL Parquet table 的时候，对 Hive metastore schema 做调整，调整规则如下：</p>
<ol>
<li>两种 schema 中字段名和字段类型必须一致（不考虑 nullable）。调和后的字段类型必须在 Parquet 格式中有相对应的数据类型，所以 nullable 是也是需要考虑的。</li>
<li>调和后 Spark SQL Parquet table schema 将包含以下字段：<ul>
<li>只出现在 Parquet schema 中的字段将被丢弃</li>
<li>只出现在 Hive metastore schema 中的字段将被添加进来，并显式地设为 nullable。</li>
</ul>
</li>
</ol>
<h4 id="刷新元数据"><a href="#刷新元数据" class="headerlink" title="刷新元数据"></a>刷新元数据</h4><p>Spark SQL 会缓存 Parquet 元数据以提高性能。如果 Hive metastore Parquet table 转换被启用的话，那么转换过来的 schema 也会被缓存。这时候，如果这些表由 Hive 或其他外部工具更新了，你必须手动刷新元数据。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_12" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_12" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_12" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_sql_12" target="_blank" rel="noopener"><strong>Sql</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 注意，这里sqlContext是一个HiveContext</span><br><span class="line">sqlContext.refreshTable(&quot;my_table&quot;)</span><br></pre></td></tr></table></figure>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>Parquet 配置可以通过 SQLContext.setConf 或者 SQL 语句中 SET key=value 来指定。</p>
<p>| 属性名 | 默认值 | 含义 |<br>| <code>spark.sql.parquet.binaryAsString</code> | false | 有些老系统，如：特定版本的 Impala，Hive，或者老版本的 Spark SQL，不区分二进制数据和字符串类型数据。这个标志的意思是，让 Spark SQL 把二进制数据当字符串处理，以兼容老系统。 |<br>| <code>spark.sql.parquet.int96AsTimestamp</code> | true | 有些老系统，如：特定版本的 Impala，Hive，把时间戳存成 INT96。这个配置的作用是，让 Spark SQL 把这些 INT96 解释为 timestamp，以兼容老系统。 |<br>| <code>spark.sql.parquet.cacheMetadata</code> | true | 缓存 Parquet schema 元数据。可以提升查询静态数据的速度。 |<br>| <code>spark.sql.parquet.compression.codec</code> | gzip | 设置 Parquet 文件的压缩编码格式。可接受的值有：uncompressed, snappy, gzip（默认）, lzo |<br>| <code>spark.sql.parquet.filterPushdown</code> | true | 启用过滤器下推优化，可以讲过滤条件尽量推导最下层，已取得性能提升 |<br>| <code>spark.sql.hive.convertMetastoreParquet</code> | true | 如果禁用，Spark SQL 将使用 Hive SerDe，而不是内建的对 Parquet tables 的支持 |<br>| <code>spark.sql.parquet.output.committer.class</code> | <code>org.apache.parquet.hadoop.
ParquetOutputCommitter</code> | Parquet 使用的数据输出类。这个类必须是 org.apache.hadoop.mapreduce.OutputCommitter 的子类。一般来说，它也应该是 org.apache.parquet.hadoop.ParquetOutputCommitter 的子类。注意：1. 如果启用 spark.speculation, 这个选项将被自动忽略</p>
<p>2. 这个选项必须用 hadoop configuration 设置，而不是 Spark SQLConf</p>
<p>3. 这个选项会覆盖 spark.sql.sources.outputCommitterClass</p>
<p>Spark SQL 有一个内建的 org.apache.spark.sql.parquet.DirectParquetOutputCommitter, 这个类的在输出到 S3 的时候比默认的 ParquetOutputCommitter 类效率高。</p>
<p> |<br>| <code>spark.sql.parquet.mergeSchema</code> | <code>false</code> | 如果设为 true，那么 Parquet 数据源将会 merge 所有数据文件的 schema，否则，schema 是从 summary file 获取的（如果 summary file 没有设置，则随机选一个） |</p>
<h2 id="JSON-数据集"><a href="#JSON-数据集" class="headerlink" title="JSON 数据集"></a>JSON 数据集</h2><ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_13" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_13" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_13" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_13" target="_blank" rel="noopener"><strong>R</strong></a></li>
<li><strong><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_sql_13" target="_blank" rel="noopener">Sql</a></strong></li>
</ul>
<p>Spark SQL 在加载 JSON 数据的时候，可以自动推导其 schema 并返回 DataFrame。用 SQLContext.read.json 读取一个包含 String 的 RDD 或者 JSON 文件，即可实现这一转换。</p>
<p>注意，通常所说的 json 文件只是包含一些 json 数据的文件，而不是我们所需要的 JSON 格式文件。JSON 格式文件必须每一行是一个独立、完整的的 JSON 对象。因此，一个常规的多行 json 文件经常会加载失败。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">// sc是已有的SparkContext对象</span><br><span class="line">val sqlContext = new org.apache.spark.sql.SQLContext(sc)</span><br><span class="line"></span><br><span class="line">// 数据集是由路径指定的</span><br><span class="line">// 路径既可以是单个文件，也可以还是存储文本文件的目录</span><br><span class="line">val path = &quot;examples/src/main/resources/people.json&quot;</span><br><span class="line">val people = sqlContext.read.json(path)</span><br><span class="line"></span><br><span class="line">// 推导出来的schema，可由printSchema打印出来</span><br><span class="line">people.printSchema()</span><br><span class="line">// root</span><br><span class="line">//  |-- age: integer (nullable = true)</span><br><span class="line">//  |-- name: string (nullable = true)</span><br><span class="line"></span><br><span class="line">// 将DataFrame注册为table</span><br><span class="line">people.registerTempTable(&quot;people&quot;)</span><br><span class="line"></span><br><span class="line">// 跑SQL语句吧！</span><br><span class="line">val teenagers = sqlContext.sql(&quot;SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19&quot;)</span><br><span class="line"></span><br><span class="line">// 另一种方法是，用一个包含JSON字符串的RDD来创建DataFrame</span><br><span class="line">val anotherPeopleRDD = sc.parallelize(</span><br><span class="line">  &quot;&quot;&quot;&#123;&quot;name&quot;:&quot;Yin&quot;,&quot;address&quot;:&#123;&quot;city&quot;:&quot;Columbus&quot;,&quot;state&quot;:&quot;Ohio&quot;&#125;&#125;&quot;&quot;&quot; :: Nil)</span><br><span class="line">val anotherPeople = sqlContext.read.json(anotherPeopleRDD)</span><br></pre></td></tr></table></figure>
<h2 id="Hive-表"><a href="#Hive-表" class="headerlink" title="Hive 表"></a>Hive 表</h2><p>Spark SQL 支持从 <a href="http://hive.apache.org/" target="_blank" rel="noopener">Apache Hive</a> 读写数据。然而，Hive 依赖项太多，所以没有把 Hive 包含在默认的 Spark 发布包里。要支持 Hive，需要在编译 spark 的时候增加 - Phive 和 - Phive-thriftserver 标志。这样编译打包的时候将会把 Hive 也包含进来。注意，hive 的 jar 包也必须出现在所有的 worker 节点上，访问 Hive 数据时候会用到（如：使用 hive 的序列化和反序列化 SerDes 时）。</p>
<p>Hive 配置在 conf / 目录下 hive-site.xml，core-site.xml（安全配置），hdfs-site.xml（HDFS 配置）文件中。请注意，如果在 YARN cluster（yarn-cluster mode）模式下执行一个查询的话，lib_mananged/jar / 下面的 datanucleus 的 jar 包，和 conf / 下的 hive-site.xml 必须在驱动器（driver）和所有执行器（executor）都可用。一种简便的方法是，通过 spark-submit 命令的–jars 和–file 选项来提交这些文件。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_14" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_14" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_14" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_14" target="_blank" rel="noopener"><strong>R</strong></a></li>
</ul>
<p>如果使用 Hive，则必须构建一个 HiveContext，HiveContext 是派生于 SQLContext 的，添加了在 Hive Metastore 里查询表的支持，以及对 HiveQL 的支持。用户没有现有的 Hive 部署，也可以创建一个 HiveContext。如果没有在 hive-site.xml 里配置，那么 HiveContext 将会自动在当前目录下创建一个 metastore_db 目录，再根据 HiveConf 设置创建一个 warehouse 目录（默认 / user/hive/warehourse）。所以请注意，你必须把 / user/hive/warehouse 的写权限赋予启动 spark 应用程序的用户。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// sc是一个已有的SparkContext对象</span><br><span class="line">val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)</span><br><span class="line"></span><br><span class="line">sqlContext.sql(&quot;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&quot;)</span><br><span class="line">sqlContext.sql(&quot;LOAD DATA LOCAL INPATH &apos;examples/src/main/resources/kv1.txt&apos; INTO TABLE src&quot;)</span><br><span class="line"></span><br><span class="line">// 这里用的是HiveQL</span><br><span class="line">sqlContext.sql(&quot;FROM src SELECT key, value&quot;).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h3 id="和不同版本的-Hive-Metastore-交互"><a href="#和不同版本的-Hive-Metastore-交互" class="headerlink" title="和不同版本的 Hive Metastore 交互"></a>和不同版本的 Hive Metastore 交互</h3><p>Spark SQL 对 Hive 最重要的支持之一就是和 Hive metastore 进行交互，这使得 Spark SQL 可以访问 Hive 表的元数据。从 Spark-1.4.0 开始，Spark SQL 有专门单独的二进制 build 版本，可以用来访问不同版本的 Hive metastore，其配置表如下。注意，不管所访问的 hive 是什么版本，Spark SQL 内部都是以 Hive 1.2.1 编译的，而且内部使用的 Hive 类也是基于这个版本（serdes，UDFs，UDAFs 等）</p>
<p>以下选项可用来配置 Hive 版本以便访问其元数据：</p>
<p>| 属性名 | 默认值 | 含义 |<br>| <code>spark.sql.hive.metastore.version</code> | <code>1.2.1</code> | Hive metastore 版本，可选的值为 0.12.0 到 1.2.1 |<br>| <code>spark.sql.hive.metastore.jars</code> | <code>builtin</code> | 初始化 HiveMetastoreClient 的 jar 包。这个属性可以是以下三者之一：</p>
<ol>
<li><code>builtin</code></li>
</ol>
<p>目前内建为使用 Hive-1.2.1，编译的时候启用 - Phive，则会和 spark 一起打包。如果没有 - Phive，那么 spark.sql.hive.metastore.version 要么是 1.2.1，要就是未定义</p>
<ol>
<li><code>maven</code></li>
</ol>
<p>使用 maven 仓库下载的 jar 包版本。这个选项建议不要再生产环境中使用</p>
<ol>
<li><p>JVM 格式的 classpath。这个 classpath 必须包含所有 Hive 及其依赖的 jar 包，且包含正确版本的 hadoop。这些 jar 包必须部署在 driver 节点上，如果你使用 yarn-cluster 模式，那么必须确保这些 jar 包也随你的应用程序一起打包</p>
<p>|<br>| <code>spark.sql.hive.metastore.sharedPrefixes</code> | <code>com.mysql.jdbc,
org.postgresql,
com.microsoft.sqlserver,
oracle.jdbc</code> | 一个逗号分隔的类名前缀列表，这些类使用 classloader 加载，且可以在 Spark SQL 和特定版本的 Hive 间共享。例如，用来访问 hive metastore 的 JDBC 的 driver 就需要这种共享。其他需要共享的类，是与某些已经共享的类有交互的类。例如，自定义的 log4j appender |<br>| <code>spark.sql.hive.metastore.barrierPrefixes</code> | <code>(empty)</code> | 一个逗号分隔的类名前缀列表，这些类在每个 Spark SQL 所访问的 Hive 版本中都会被显式的 reload。例如，某些在共享前缀列表（spark.sql.hive.metastore.sharedPrefixes）中声明为共享的 Hive UD 函数 |</p>
</li>
</ol>
<h2 id="用-JDBC-连接其他数据库"><a href="#用-JDBC-连接其他数据库" class="headerlink" title="用 JDBC 连接其他数据库"></a>用 JDBC 连接其他数据库</h2><p>Spark SQL 也可以用 JDBC 访问其他数据库。这一功能应该优先于使用 JdbcRDD。因为它返回一个 DataFrame，而 DataFrame 在 Spark SQL 中操作更简单，且更容易和来自其他数据源的数据进行交互关联。JDBC 数据源在 java 和 python 中用起来也很简单，不需要用户提供额外的 ClassTag。（注意，这与 Spark SQL JDBC server 不同，Spark SQL JDBC server 允许其他应用执行 Spark SQL 查询）</p>
<p>首先，你需要在 spark classpath 中包含对应数据库的 JDBC driver，下面这行包括了用于访问 postgres 的数据库 driver</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SPARK_CLASSPATH=postgresql-9.3-1102-jdbc41.jar bin/spark-shell</span><br></pre></td></tr></table></figure>
<p>远程数据库的表可以通过 Data Sources API，用 DataFrame 或者 SparkSQL 临时表来装载。以下是选项列表：</p>
<p>| 属性名 | 含义 |<br>| <code>url</code> | 需要连接的 JDBC URL |<br>| <code>dbtable</code> | 需要读取的 JDBC 表。注意，任何可以填在 SQL 的 where 子句中的东西，都可以填在这里。（既可以填完整的表名，也可填括号括起来的子查询语句） |<br>| <code>driver</code> | JDBC driver 的类名。这个类必须在 master 和 worker 节点上都可用，这样各个节点才能将 driver 注册到 JDBC 的子系统中。 |<br>| <code>partitionColumn, lowerBound, upperBound, numPartitions</code> | 这几个选项，如果指定其中一个，则必须全部指定。他们描述了多个 worker 如何并行的读入数据，并将表分区。partitionColumn 必须是所查询的表中的一个数值字段。注意，lowerBound 和 upperBound 只是用于决定分区跨度的，而不是过滤表中的行。因此，表中所有的行都会被分区然后返回。 |<br>| <code>fetchSize</code> | JDBC fetch size，决定每次获取多少行数据。在 JDBC 驱动上设成较小的值有利于性能优化（如，Oracle 上设为 10） |</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_15" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_15" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_15" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_15" target="_blank" rel="noopener"><strong>R</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_sql_15" target="_blank" rel="noopener"><strong>Sql</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val jdbcDF = sqlContext.read.format(&quot;jdbc&quot;).options(</span><br><span class="line">  Map(&quot;url&quot; -&gt; &quot;jdbc:postgresql:dbserver&quot;,</span><br><span class="line">  &quot;dbtable&quot; -&gt; &quot;schema.tablename&quot;)).load()</span><br></pre></td></tr></table></figure>
<h2 id="疑难解答"><a href="#疑难解答" class="headerlink" title="疑难解答"></a>疑难解答</h2><ul>
<li>JDBC driver class 必须在所有 client session 或者 executor 上，对 java 的原生 classloader 可见。这是因为 Java 的 DriverManager 在打开一个连接之前，会做安全检查，并忽略所有对原声 classloader 不可见的 driver。最简单的一种方法，就是在所有 worker 节点上修改 compute_classpath.sh，并包含你所需的 driver jar 包。</li>
<li>一些数据库，如 H2，会把所有的名字转大写。对于这些数据库，在 Spark SQL 中必须也使用大写。</li>
</ul>
<h1 id="性能调整"><a href="#性能调整" class="headerlink" title="性能调整"></a>性能调整</h1><p>对于有一定计算量的 Spark 作业来说，可能的性能改进的方式，不是把数据缓存在内存里，就是调整一些开销较大的选项参数。</p>
<h2 id="内存缓存"><a href="#内存缓存" class="headerlink" title="内存缓存"></a>内存缓存</h2><p>Spark SQL 可以通过调用 SQLContext.cacheTable(“tableName”) 或者 DataFrame.cache() 把 tables 以列存储格式缓存到内存中。随后，Spark SQL 将会扫描必要的列，并自动调整压缩比例，以减少内存占用和 GC 压力。你也可以用 SQLContext.uncacheTable(“tableName”) 来删除内存中的 table。</p>
<p>你还可以使用 SQLContext.setConf 或在 SQL 语句中运行 SET key=value 命令，来配置内存中的缓存。</p>
<p>| 属性名 | 默认值 | 含义 |<br>| <code>spark.sql.inMemoryColumnarStorage.compressed</code> | true | 如果设置为 true，Spark SQL 将会根据数据统计信息，自动为每一列选择单独的压缩编码方式。 |<br>| <code>spark.sql.inMemoryColumnarStorage.batchSize</code> | 10000 | 控制列式缓存批量的大小。增大批量大小可以提高内存利用率和压缩率，但同时也会带来 OOM（Out Of Memory）的风险。 |</p>
<h2 id="其他配置选项"><a href="#其他配置选项" class="headerlink" title="其他配置选项"></a>其他配置选项</h2><p>以下选项同样也可以用来给查询任务调性能。不过这些选项在未来可能被放弃，因为 spark 将支持越来越多的自动优化。</p>
<p>| 属性名 | 默认值 | 含义 |<br>| <code>spark.sql.autoBroadcastJoinThreshold</code> | 10485760 (10 MB) | 配置 join 操作时，能够作为广播变量的最大 table 的大小。设置为 - 1，表示禁用广播。注意，目前的元数据统计仅支持 Hive metastore 中的表，并且需要运行这个命令：ANALYSE TABLE <tablename> COMPUTE STATISTICS noscan |<br>| <code>spark.sql.tungsten.enabled</code> | true | 设为 true，则启用优化的 Tungsten 物理执行后端。Tungsten 会显式的管理内存，并动态生成表达式求值的字节码 |<br>| <code>spark.sql.shuffle.partitions</code> | 200 | 配置数据混洗（shuffle）时（join 或者聚合操作），使用的分区数。 |</tablename></p>
<h1 id="分布式-SQL-引擎"><a href="#分布式-SQL-引擎" class="headerlink" title="分布式 SQL 引擎"></a>分布式 SQL 引擎</h1><p>Spark SQL 可以作为 JDBC/ODBC 或者命令行工具的分布式查询引擎。在这种模式下，终端用户或应用程序，无需写任何代码，就可以直接在 Spark SQL 中运行 SQL 查询。</p>
<h2 id="运行-Thrift-JDBC-ODBC-server"><a href="#运行-Thrift-JDBC-ODBC-server" class="headerlink" title="运行 Thrift JDBC/ODBC server"></a>运行 Thrift JDBC/ODBC server</h2><p>这里实现的 Thrift JDBC/ODBC server 和 Hive-1.2.1 中的 <a href="https://cwiki.apache.org/confluence/display/Hive/Setting+Up+HiveServer2" target="_blank" rel="noopener"><code>HiveServer2</code></a>是相同的。你可以使用 beeline 脚本来测试 Spark 或者 Hive-1.2.1 的 JDBC server。</p>
<p>在 Spark 目录下运行下面这个命令，启动一个 JDBC/ODBC server</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh</span><br></pre></td></tr></table></figure>
<p>这个脚本能接受所有 bin/spark-submit 命令支持的选项参数，外加一个 –hiveconf 选项，来指定 Hive 属性。运行./sbin/start-thriftserver.sh –help 可以查看完整的选项列表。默认情况下，启动的 server 将会在 localhost:10000 端口上监听。要改变监听主机名或端口，可以用以下环境变量：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;</span><br><span class="line">export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;</span><br><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">  --master &lt;master-uri&gt; \</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>或者 Hive 系统属性 来指定</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">  --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">  --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">  --master &lt;master-uri&gt;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>接下来，你就可以开始在 beeline 中测试这个 Thrift JDBC/ODBC server:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/beeline</span><br></pre></td></tr></table></figure>
<p>下面的指令，可以连接到一个 JDBC/ODBC server</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://localhost:10000</span><br></pre></td></tr></table></figure>
<p>可能需要输入用户名和密码。在非安全模式下，只要输入你本机的用户名和一个空密码即可。对于安全模式，请参考 <a href="https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients" target="_blank" rel="noopener">beeline documentation</a>.</p>
<p>Hive 的配置是在 conf / 目录下的 hive-site.xml，core-site.xml，hdfs-site.xml 中指定的。</p>
<p>你也可以在 beeline 的脚本中指定。</p>
<p>Thrift JDBC server 也支持通过 HTTP 传输 Thrift RPC 消息。以下配置（在 conf/hive-site.xml 中）将启用 HTTP 模式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive.server2.transport.mode - Set this to value: http</span><br><span class="line">hive.server2.thrift.http.port - HTTP port number fo listen on; default is 10001</span><br><span class="line">hive.server2.http.endpoint - HTTP endpoint; default is cliservice</span><br></pre></td></tr></table></figure>
<p>同样，在 beeline 中也可以用 HTTP 模式连接 JDBC/ODBC server:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">beeline&gt; !connect jdbc:hive2://&lt;host&gt;:&lt;port&gt;/&lt;database&gt;?hive.server2.transport.mode=http;hive.server2.thrift.http.path=&lt;http_endpoint&gt;</span><br></pre></td></tr></table></figure>
<h2 id="使用-Spark-SQL-命令行工具"><a href="#使用-Spark-SQL-命令行工具" class="headerlink" title="使用 Spark SQL 命令行工具"></a>使用 Spark SQL 命令行工具</h2><p>Spark SQL CLI 是一个很方便的工具，它可以用 local mode 运行 hive metastore service，并且在命令行中执行输入的查询。注意 Spark SQL CLI 目前还不支持和 Thrift JDBC server 通信。</p>
<p>用如下命令，在 spark 目录下启动一个 Spark SQL CLI</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-sql</span><br></pre></td></tr></table></figure>
<p>Hive 配置在 conf 目录下 hive-site.xml，core-site.xml，hdfs-site.xml 中设置。你可以用这个命令查看完整的选项列表：./bin/spark-sql –help</p>
<h1 id="升级指南"><a href="#升级指南" class="headerlink" title="升级指南"></a>升级指南</h1><h2 id="1-5-升级到-1-6"><a href="#1-5-升级到-1-6" class="headerlink" title="1.5 升级到 1.6"></a>1.5 升级到 1.6</h2><ul>
<li>从 Spark-1.6.0 起，默认 Thrift server 将运行于多会话并存模式下（multi-session）。这意味着，每个 JDBC/ODBC 连接有其独立的 SQL 配置和临时函数注册表。table 的缓存仍然是公用的。如果你更喜欢老的单会话模式，只需设置 spark.sql.hive.thriftServer.singleSession 为 true 即可。当然，你也可在 spark-defaults.conf 中设置，或者将其值传给 start-thriftserver.sh –conf（如下）：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">     --conf spark.sql.hive.thriftServer.singleSession=true \</span><br><span class="line">     ...</span><br></pre></td></tr></table></figure>
<h2 id="1-4-升级到-1-5"><a href="#1-4-升级到-1-5" class="headerlink" title="1.4 升级到 1.5"></a>1.4 升级到 1.5</h2><ul>
<li>Tungsten 引擎现在默认是启用的，Tungsten 是通过手动管理内存优化执行计划，同时也优化了表达式求值的代码生成。这两个特性都可以通过把 spark.sql.tungsten.enabled 设为 false 来禁用。</li>
<li>Parquet schema merging 默认不启用。需要启用的话，设置 spark.sql.parquet.mergeSchema 为 true 即可</li>
<li>Python 接口支持用点 (.) 来访问字段内嵌值，例如 df[‘table.column.nestedField’]。但这也意味着，如果你的字段名包含点号 (.) 的话，你就必须用重音符来转义，如：table.<code>column.with.dots</code>.nested。</li>
<li>列式存储内存分区剪枝默认是启用的。要禁用，设置 spark.sql.inMemoryColumarStorage.partitionPruning 为 false 即可</li>
<li>不再支持无精度限制的 decimal。Spark SQL 现在强制最大精度为 38 位。对于 BigDecimal 对象，类型推导将会使用（38，18）精度的 decimal 类型。如果 DDL 中没有指明精度，默认使用的精度是（10，0）</li>
<li>时间戳精确到 1us（微秒），而不是 1ns（纳秒）</li>
<li>在 “sql” 这个 SQL 变种设置中，浮点数将被解析为 decimal。HiveQL 解析保持不变。</li>
<li>标准 SQL/DataFrame 函数均为小写，例如：sum vs SUM。</li>
<li>当推测任务被启用是，使用 DirectOutputCommitter 是不安全的，因此，DirectOutputCommitter 在推测任务启用时，将被自动禁用，且忽略相关配置。</li>
<li>JSON 数据源不再自动加载其他程序产生的新文件（例如，不是 Spark SQL 插入到 dataset 中的文件）。对于一个 JSON 的持久化表（如：Hive metastore 中保存的表），用户可以使用 REFRESH TABLE 这个 SQL 命令或者 HiveContext.refreshTable 来把新文件包括进来。</li>
</ul>
<h2 id="1-3-升级到-1-4"><a href="#1-3-升级到-1-4" class="headerlink" title="1.3 升级到 1.4"></a>1.3 升级到 1.4</h2><h4 id="DataFrame-数据读写接口"><a href="#DataFrame-数据读写接口" class="headerlink" title="DataFrame 数据读写接口"></a>DataFrame 数据读写接口</h4><p>根据用户的反馈，我们提供了一个新的，更加流畅的 API，用于数据读（SQLContext.read）写（DataFrame.write），同时老的 API（如：SQLCOntext.parquetFile, SQLContext.jsonFile）将被废弃。</p>
<p>有关 SQLContext.read 和 DataFrame.write 的更详细信息，请参考 API 文档。</p>
<h4 id="DataFrame-groupBy-保留分组字段"><a href="#DataFrame-groupBy-保留分组字段" class="headerlink" title="DataFrame.groupBy 保留分组字段"></a>DataFrame.groupBy 保留分组字段</h4><p>根据用户的反馈，我们改变了 DataFrame.groupBy().agg() 的默认行为，在返回的 DataFrame 结果中保留了分组字段。如果你想保持 1.3 中的行为，设置 spark.sql.retainGroupColumns 为 false 即可。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_16" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_16" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_16" target="_blank" rel="noopener"><strong>Python</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 在1.3.x中，如果要保留分组字段&quot;department&quot;, 你必须显式的在agg聚合时包含这个字段</span><br><span class="line">df.groupBy(&quot;department&quot;).agg($&quot;department&quot;, max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line"></span><br><span class="line">// 而在1.4+，分组字段&quot;department&quot;默认就会包含在返回的DataFrame中</span><br><span class="line">df.groupBy(&quot;department&quot;).agg(max(&quot;age&quot;), sum(&quot;expense&quot;))</span><br><span class="line"></span><br><span class="line">// 要回滚到1.3的行为（不包含分组字段），按如下设置即可：</span><br><span class="line">sqlContext.setConf(&quot;spark.sql.retainGroupColumns&quot;, &quot;false&quot;)</span><br></pre></td></tr></table></figure>
<h2 id="1-2-升级到-1-3"><a href="#1-2-升级到-1-3" class="headerlink" title="1.2 升级到 1.3"></a>1.2 升级到 1.3</h2><p>在 Spark 1.3 中，我们去掉了 Spark SQL 的”Alpha“标签，并清理了可用的 API。从 Spark 1.3 起，Spark SQL 将对 1.x 系列二进制兼容。这个兼容性保证不包括显式的标注为”unstable（如：DeveloperAPI 或 Experimental）“的 API。</p>
<h4 id="SchemaRDD-重命名为-DataFrame"><a href="#SchemaRDD-重命名为-DataFrame" class="headerlink" title="SchemaRDD 重命名为 DataFrame"></a>SchemaRDD 重命名为 DataFrame</h4><p>对于用户来说，Spark SQL 1.3 最大的改动就是 SchemaRDD 改名为 DataFrame。主要原因是，DataFrame 不再直接由 RDD 派生，而是通过自己的实现提供 RDD 的功能。DataFrame 只需要调用其 rdd 方法就能转成 RDD。</p>
<p>在 Scala 中仍然有 SchemaRDD，只不过这是 DataFrame 的一个别名，以便兼容一些现有代码。但仍然建议用户改用 DataFrame。Java 和 Python 用户就没这个福利了，他们必须改代码。</p>
<h4 id="统一-Java-和-Scala-API"><a href="#统一-Java-和-Scala-API" class="headerlink" title="统一 Java 和 Scala API"></a>统一 Java 和 Scala API</h4><p>在 Spark 1.3 之前，有单独的 java 兼容类（JavaSQLContext 和 JavaSchemaRDD）及其在 Scala API 中的镜像。Spark 1.3 中将 Java API 和 Scala API 统一。两种语言的用户都应该使用 SQLContext 和 DataFrame。一般这些类中都会使用两种语言中都有的类型（如：Array 取代各语言独有的集合）。有些情况下，没有通用的类型（例如：闭包或者 maps），将会使用函数重载来解决这个问题。</p>
<p>另外，java 特有的类型 API 被删除了。Scala 和 java 用户都应该用 org.apache.spark.sql.types 来编程描述一个 schema。</p>
<h4 id="隐式转换隔离，DSL-包移除-–-仅针对-scala"><a href="#隐式转换隔离，DSL-包移除-–-仅针对-scala" class="headerlink" title="隐式转换隔离，DSL 包移除 – 仅针对 scala"></a>隐式转换隔离，DSL 包移除 – 仅针对 scala</h4><p>Spark 1.3 之前的很多示例代码，都在开头用 import sqlContext.<em>，这行将会导致所有的 sqlContext 的函数都被引入进来。因此，在 Spark 1.3 我们把 RDDs 到 DataFrames 的隐式转换隔离出来，单独放到 SQLContext.implicits 对象中。用户现在应该这样写：import sqlContext.implicits.</em></p>
<p>另外，隐式转换也支持由 Product（如：case classes 或 tuples）组成的 RDD，但需要调用一个 toDF 方法，而不是自动转换。</p>
<p>如果需要使用 DSL（被 DataFrame 取代的 API）中的方法，用户之前需要导入 DSL（import org.apache.spark.sql.catalyst.dsl）， 而现在应该要导入 DataFrame API（import org.apache.spark.sql.functions._）</p>
<h4 id="移除-org-apache-spark-sql-中-DataType-别名-–-仅针对-scala"><a href="#移除-org-apache-spark-sql-中-DataType-别名-–-仅针对-scala" class="headerlink" title="移除 org.apache.spark.sql 中 DataType 别名 – 仅针对 scala"></a>移除 org.apache.spark.sql 中 DataType 别名 – 仅针对 scala</h4><p>Spark 1.3 删除了 sql 包中的 DataType 类型别名。现在，用户应该使用 org.apache.spark.sql.types 中的类。</p>
<h4 id="UDF-注册挪到-sqlContext-udf-中-–-针对-java-和-scala"><a href="#UDF-注册挪到-sqlContext-udf-中-–-针对-java-和-scala" class="headerlink" title="UDF 注册挪到 sqlContext.udf 中 – 针对 java 和 scala"></a>UDF 注册挪到 sqlContext.udf 中 – 针对 java 和 scala</h4><p>注册 UDF 的函数，不管是 DataFrame，DSL 或者 SQL 中用到的，都被挪到 SQLContext.udf 中。</p>
<ul>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_17" target="_blank" rel="noopener"><strong>Scala</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_17" target="_blank" rel="noopener"><strong>Java</strong></a></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.udf.register(&quot;strLen&quot;, (s: String) =&gt; s.length())</span><br></pre></td></tr></table></figure>
<p>Python UDF 注册保持不变。</p>
<h4 id="Python-DataTypes-不再是单例"><a href="#Python-DataTypes-不再是单例" class="headerlink" title="Python DataTypes 不再是单例"></a>Python DataTypes 不再是单例</h4><p>在 python 中使用 DataTypes，你需要先构造一个对象（如：StringType()），而不是引用一个单例。</p>
<h2 id="Shark-用户迁移指南"><a href="#Shark-用户迁移指南" class="headerlink" title="Shark 用户迁移指南"></a>Shark 用户迁移指南</h2><h3 id="调度"><a href="#调度" class="headerlink" title="调度"></a>调度</h3><p>用户可以通过如下命令，为 JDBC 客户端 session 设定一个 <a href="http://spark.apache.org/docs/latest/job-scheduling.html#fair-scheduler-pools" target="_blank" rel="noopener">Fair Scheduler</a> pool。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SET spark.sql.thriftserver.scheduler.pool=accounting;</span><br></pre></td></tr></table></figure>
<h3 id="Reducer-个数"><a href="#Reducer-个数" class="headerlink" title="Reducer 个数"></a>Reducer 个数</h3><p>在 Shark 中，默认的 reducer 个数是 1，并且由 mapred.reduce.tasks 设定。Spark SQL 废弃了这个属性，改为 spark.sql.shuffle.partitions, 并且默认 200，用户可通过如下 SET 命令来自定义：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SET spark.sql.shuffle.partitions=10;</span><br><span class="line">SELECT page, count(*) c</span><br><span class="line">FROM logs_last_month_cached</span><br><span class="line">GROUP BY page ORDER BY c DESC LIMIT 10;</span><br></pre></td></tr></table></figure>
<p>你也可以把这个属性放到 hive-site.xml 中来覆盖默认值。</p>
<p>目前，mapred.reduce.tasks 属性仍然能被识别，并且自动转成 spark.sql.shuffle.partitions</p>
<h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>shark.cache 表属性已经不存在了，并且以”_cached” 结尾命名的表也不再会自动缓存。取而代之的是，CACHE TABLE 和 UNCACHE TABLE 语句，用以显式的控制表的缓存：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CACHE TABLE logs_last_month;</span><br><span class="line">UNCACHE TABLE logs_last_month;</span><br></pre></td></tr></table></figure>
<p>注意：CACHE TABLE tbl 现在默认是饥饿模式，而非懒惰模式。再也不需要手动调用其他 action 来触发 cache 了！</p>
<p>从 Spark-1.2.0 开始，Spark SQL 新提供了一个语句，让用户自己控制表缓存是否是懒惰模式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CACHE [LAZY] TABLE [AS SELECT] ...</span><br></pre></td></tr></table></figure>
<p>以下几个缓存相关的特性不再支持：</p>
<ul>
<li>用户定义分区级别的缓存逐出策略</li>
<li>RDD 重加载</li>
<li>内存缓存直接写入策略</li>
</ul>
<h2 id="兼容-Apache-Hive"><a href="#兼容-Apache-Hive" class="headerlink" title="兼容 Apache Hive"></a>兼容 Apache Hive</h2><p>Spark SQL 设计时考虑了和 Hive metastore，SerDes 以及 UDF 的兼容性。目前这些兼容性斗是基于 Hive-1.2.1 版本，并且 Spark SQL 可以连到不同版本的 Hive metastore（从 0.12.0 到 1.2.1，参考：<a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore</a>）</p>
<h4 id="部署在已有的-Hive-仓库之上"><a href="#部署在已有的-Hive-仓库之上" class="headerlink" title="部署在已有的 Hive 仓库之上"></a>部署在已有的 Hive 仓库之上</h4><p>Spark SQL Thrift JDBC server 采用了”out of the box”（开箱即用）的设计，使用很方便，并兼容已有的 Hive 安装版本。你不需要修改已有的 Hive metastore 或者改变数据的位置，或者表分区。</p>
<h3 id="支持的-Hive-功能"><a href="#支持的-Hive-功能" class="headerlink" title="支持的 Hive 功能"></a>支持的 Hive 功能</h3><p>Spark SQL 支持绝大部分 Hive 功能，如：</p>
<ul>
<li>Hive 查询语句：<ul>
<li><code>SELECT</code></li>
<li><code>GROUP BY</code></li>
<li><code>ORDER BY</code></li>
<li><code>CLUSTER BY</code></li>
<li><code>SORT BY</code></li>
</ul>
</li>
<li>所有的 Hive 操作符：<ul>
<li>Relational operators (<code>=</code>, <code>⇔</code>, <code>==</code>, <code>&lt;&gt;</code>, <code>&lt;</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, etc)</li>
<li>Arithmetic operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>%</code>, etc)</li>
<li>Logical operators (<code>AND</code>, <code>&amp;&amp;</code>, <code>OR</code>, <code>||</code>, etc)</li>
<li>Complex type constructors</li>
<li>Mathematical functions (<code>sign</code>, <code>ln</code>, <code>cos</code>, etc)</li>
<li>String functions (<code>instr</code>, <code>length</code>, <code>printf</code>, etc)</li>
</ul>
</li>
<li>用户定义函数（UDF）</li>
<li>用户定义聚合函数（UDAF）</li>
<li>用户定义序列化、反序列化（SerDes）</li>
<li>窗口函数（Window functions）</li>
<li>Joins<ul>
<li><code>JOIN</code></li>
<li><code>{LEFT|RIGHT|FULL} OUTER JOIN</code></li>
<li><code>LEFT SEMI JOIN</code></li>
<li><code>CROSS JOIN</code></li>
</ul>
</li>
<li>Unions</li>
<li>查询子句<ul>
<li><code>SELECT col FROM ( SELECT a + b AS col from t1) t2</code></li>
</ul>
</li>
<li>采样</li>
<li>执行计划详细（Explain）</li>
<li>分区表，包括动态分区插入</li>
<li>视图</li>
<li>所有 Hive DDL（data definition language）：<ul>
<li><code>CREATE TABLE</code></li>
<li><code>CREATE TABLE AS SELECT</code></li>
<li><code>ALTER TABLE</code></li>
</ul>
</li>
<li>绝大部分 Hive 数据类型：<ul>
<li><code>TINYINT</code></li>
<li><code>SMALLINT</code></li>
<li><code>INT</code></li>
<li><code>BIGINT</code></li>
<li><code>BOOLEAN</code></li>
<li><code>FLOAT</code></li>
<li><code>DOUBLE</code></li>
<li><code>STRING</code></li>
<li><code>BINARY</code></li>
<li><code>TIMESTAMP</code></li>
<li><code>DATE</code></li>
<li><code>ARRAY&lt;&gt;</code></li>
<li><code>MAP&lt;&gt;</code></li>
<li><code>STRUCT&lt;&gt;</code></li>
</ul>
</li>
</ul>
<h3 id="不支持的-Hive-功能"><a href="#不支持的-Hive-功能" class="headerlink" title="不支持的 Hive 功能"></a>不支持的 Hive 功能</h3><p>以下是目前不支持的 Hive 特性的列表。多数是不常用的。</p>
<p><strong>不支持的</strong> <strong>Hive</strong> <strong>常见功能</strong></p>
<ul>
<li>bucket 表：butcket 是 Hive 表的一个哈希分区</li>
</ul>
<p><strong>不支持的</strong> <strong>Hive</strong> <strong>高级**</strong>功能**</p>
<ul>
<li>UNION 类操作</li>
<li>去重 join</li>
<li>字段统计信息收集：Spark SQL 不支持同步的字段统计收集</li>
</ul>
<p><strong>Hive 输入、输出格式</strong></p>
<ul>
<li>CLI 文件格式：对于需要回显到 CLI 中的结果，Spark SQL 仅支持 TextOutputFormat。</li>
<li>Hadoop archive — Hadoop 归档</li>
</ul>
<p><strong>Hive 优化</strong></p>
<p>一些比较棘手的 Hive 优化目前还没有在 Spark 中提供。有一些（如索引）对应 Spark SQL 这种内存计算模型来说并不重要。另外一些，在 Spark SQL 未来的版本中会支持。</p>
<ul>
<li>块级别位图索引和虚拟字段（用来建索引）</li>
<li>自动计算 reducer 个数（join 和 groupBy 算子）：目前在 Spark SQL 中你需要这样控制混洗后（post-shuffle）并发程度：”SET spark.sql.shuffle.partitions=[num_tasks];”</li>
<li>元数据查询：只查询元数据的请求，Spark SQL 仍需要启动任务来计算结果</li>
<li>数据倾斜标志：Spark SQL 不会理会 Hive 中的数据倾斜标志</li>
<li><code>STREAMTABLE</code> join 提示：Spark SQL 里没有这玩艺儿</li>
<li>返回结果时合并小文件：如果返回的结果有很多小文件，Hive 有个选项设置，来合并小文件，以避免超过 HDFS 的文件数额度限制。Spark SQL 不支持这个。</li>
</ul>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>Spark SQL 和 DataFrames 支持如下数据类型：</p>
<ul>
<li>Numeric types（数值类型）<ul>
<li><code>ByteType</code>: 1 字节长的有符号整型，范围：<code>-128</code> 到 <code>127</code>.</li>
<li><code>ShortType</code>: 2 字节长有符号整型，范围：<code>-32768</code> 到 <code>32767</code>.</li>
<li><code>IntegerType</code>: 4 字节有符号整型，范围：<code>-2147483648</code> 到 <code>2147483647</code>.</li>
<li><code>LongType</code>: 8 字节有符号整型，范围： <code>-9223372036854775808</code> to <code>9223372036854775807</code>.</li>
<li><code>FloatType</code>: 4 字节单精度浮点数。</li>
<li><code>DoubleType</code>: 8 字节双精度浮点数</li>
<li><code>DecimalType</code>: 任意精度有符号带小数的数值。内部使用 java.math.BigDecimal, BigDecimal 包含任意精度的不缩放整型，和一个 32 位的缩放整型</li>
</ul>
</li>
<li>String type（字符串类型）<ul>
<li><code>StringType</code>: 字符串</li>
</ul>
</li>
<li>Binary type（二进制类型）<ul>
<li><code>BinaryType</code>: 字节序列</li>
</ul>
</li>
<li>Boolean type（布尔类型）<ul>
<li><code>BooleanType</code>: 布尔类型</li>
</ul>
</li>
<li>Datetime type（日期类型）<ul>
<li><code>TimestampType</code>: 表示包含年月日、时分秒等字段的日期</li>
<li><code>DateType</code>: 表示包含年月日字段的日期</li>
</ul>
</li>
<li><p>Complex types（复杂类型）</p>
<ul>
<li><code>ArrayType(elementType, containsNull)</code>：数组类型，表达一系列的 elementType 类型的元素组成的序列，containsNull 表示数组能否包含 null 值</li>
<li><code>MapType(keyType, valueType, valueContainsNull)</code>：映射集合类型，表示一个键值对的集合。键的类型是 keyType，值的类型则由 valueType 指定。对应 MapType 来说，键是不能为 null 的，而值能否为 null 则取决于 valueContainsNull。</li>
<li><code>StructType(fields)：</code>表示包含 StructField 序列的结构体。<ul>
<li>StructField(name, datatype, nullable): 表示 StructType 中的一个字段，name 是字段名，datatype 是数据类型，nullable 表示该字段是否可以为空</li>
</ul>
</li>
</ul>
</li>
<li><p><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_scala_18" target="_blank" rel="noopener"><strong>Scala</strong></a></p>
</li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_java_18" target="_blank" rel="noopener"><strong>Java</strong></a></li>
<li><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_python_18" target="_blank" rel="noopener"><strong>Python</strong></a></li>
<li><strong><a href="http://spark.apache.org/docs/latest/sql-programming-guide.html#tab_r_18" target="_blank" rel="noopener">R</a></strong></li>
</ul>
<p>所有 Spark SQL 支持的数据类型都在这个包里：org.apache.spark.sql.types，你可以这样导入之：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">import  org.apache.spark.sql.types._</span><br></pre></td></tr></table></figure>
<p>| Data type | Value type in Scala | API to access or create a data type |<br>| <strong>ByteType</strong> | Byte | ByteType |<br>| <strong>ShortType</strong> | Short | ShortType |<br>| <strong>IntegerType</strong> | Int | IntegerType |<br>| <strong>LongType</strong> | Long | LongType |<br>| <strong>FloatType</strong> | Float | FloatType |<br>| <strong>DoubleType</strong> | Double | DoubleType |<br>| <strong>DecimalType</strong> | java.math.BigDecimal | DecimalType |<br>| <strong>StringType</strong> | String | StringType |<br>| <strong>BinaryType</strong> | Array[Byte] | BinaryType |<br>| <strong>BooleanType</strong> | Boolean | BooleanType |<br>| <strong>TimestampType</strong> | java.sql.Timestamp | TimestampType |<br>| <strong>DateType</strong> | java.sql.Date | DateType |<br>| <strong>ArrayType</strong> | scala.collection.Seq | ArrayType(<em>elementType</em>, [<em>containsNull</em>]) 注意：默认 containsNull 为 true |<br>| <strong>MapType</strong> | scala.collection.Map | MapType(<em>keyType</em>, <em>valueType</em>, [<em>valueContainsNull</em>]) 注意：默认 valueContainsNull 为 true |<br>| <strong>StructType</strong> | org.apache.spark.sql.Row | StructType(<em>fields</em>) 注意：fields 是一个 StructFields 的序列，并且同名的字段是不允许的。 |<br>| <strong>StructField</strong> | 定义字段的数据对应的 Scala 类型（例如，如果 StructField 的 dataType 为 IntegerType，则其数据对应的 scala 类型为 Int） | StructField(<em>name</em>, <em>dataType</em>, <em>nullable</em>) |</p>
<h2 id="NaN-语义"><a href="#NaN-语义" class="headerlink" title="NaN 语义"></a>NaN 语义</h2><p>这是 Not-a-Number 的缩写，某些 float 或 double 类型不符合标准浮点数语义，需要对其特殊处理：</p>
<ul>
<li>NaN == NaN，即：NaN 和 NaN 总是相等</li>
<li>在聚合函数中，所有 NaN 分到同一组</li>
<li>NaN 在 join 操作中可以当做一个普通的 join key</li>
<li>NaN 在升序排序中排到最后，比任何其他数值都大</li>
</ul>

      
    </div>
    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>

      
    </div>

    
      <div>
        <div id="wechat_subscriber" style="display: block; padding: 10px 0; margin: 20px auto; width: 100%; text-align: center">
    <img id="wechat_subscriber_qcode" src="/path/to/your/wechatqcode ex. /uploads/wechat-qcode.jpg" alt="Dean Wang wechat" style="width: 200px; max-width: 100%;"/>
    <div></div>
</div>

      </div>
    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/bugs/tags/大数据/" rel="tag"><i class="fa fa-tag"></i> 大数据</a>
          
            <a href="/bugs/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/bugs/2018/09/25/语言/Java/Java 8 中的 Streams API 详解 Java 8 中的 Streams API 详解/" rel="next" title="Java 8 中的 Streams API 详解 Java 8 中的 Streams API 详解">
                <i class="fa fa-chevron-left"></i> Java 8 中的 Streams API 详解 Java 8 中的 Streams API 详解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/bugs/2018/09/28/算法/漫画：什么是红黑树？/" rel="prev" title="漫画：什么是红黑树？">
                漫画：什么是红黑树？ <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Dean Wang</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/bugs/archives/">
              
                  <span class="site-state-item-count">87</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/bugs/categories/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/bugs/tags/index.html">
                  <span class="site-state-item-count">34</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/deanwang1943" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:wangjingxin1986@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Spark-SQL-DataFrames-以及-Datasets-编程指南"><span class="nav-number">1.</span> <span class="nav-text">Spark SQL, DataFrames 以及 Datasets 编程指南</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#概要"><span class="nav-number"></span> <span class="nav-text">概要</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#SQL"><span class="nav-number">1.</span> <span class="nav-text">SQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrames"><span class="nav-number">2.</span> <span class="nav-text">DataFrames</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Datasets"><span class="nav-number">3.</span> <span class="nav-text">Datasets</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#入门"><span class="nav-number"></span> <span class="nav-text">入门</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#入口：SQLContext"><span class="nav-number">1.</span> <span class="nav-text">入口：SQLContext</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-DataFrame"><span class="nav-number">2.</span> <span class="nav-text">创建 DataFrame</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DataFrame-操作"><span class="nav-number">3.</span> <span class="nav-text">DataFrame 操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#编程方式执行-SQL-查询"><span class="nav-number">4.</span> <span class="nav-text">编程方式执行 SQL 查询</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建-Dataset"><span class="nav-number">5.</span> <span class="nav-text">创建 Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#和-RDD-互操作"><span class="nav-number">6.</span> <span class="nav-text">和 RDD 互操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#利用反射推导-schema"><span class="nav-number">6.1.</span> <span class="nav-text">利用反射推导 schema</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编程方式定义-Schema"><span class="nav-number">6.2.</span> <span class="nav-text">编程方式定义 Schema</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#数据源"><span class="nav-number"></span> <span class="nav-text">数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#通用加载-保存函数"><span class="nav-number">1.</span> <span class="nav-text">通用加载 / 保存函数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#手动指定选项"><span class="nav-number">1.1.</span> <span class="nav-text">手动指定选项</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#直接对文件使用-SQL"><span class="nav-number">1.2.</span> <span class="nav-text">直接对文件使用 SQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#保存模式"><span class="nav-number">1.3.</span> <span class="nav-text">保存模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#保存到持久化表"><span class="nav-number">1.4.</span> <span class="nav-text">保存到持久化表</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Parquet-文件"><span class="nav-number">2.</span> <span class="nav-text">Parquet 文件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#编程方式加载数据"><span class="nav-number">2.1.</span> <span class="nav-text">编程方式加载数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分区发现"><span class="nav-number">2.2.</span> <span class="nav-text">分区发现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Schema-合并"><span class="nav-number">2.3.</span> <span class="nav-text">Schema 合并</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive-metastore-Parquet-table-转换"><span class="nav-number">2.4.</span> <span class="nav-text">Hive metastore Parquet table 转换</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hive-Parquet-schema-调和"><span class="nav-number">2.4.1.</span> <span class="nav-text">Hive/Parquet schema 调和</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#刷新元数据"><span class="nav-number">2.4.2.</span> <span class="nav-text">刷新元数据</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置"><span class="nav-number">2.5.</span> <span class="nav-text">配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#JSON-数据集"><span class="nav-number">3.</span> <span class="nav-text">JSON 数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hive-表"><span class="nav-number">4.</span> <span class="nav-text">Hive 表</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#和不同版本的-Hive-Metastore-交互"><span class="nav-number">4.1.</span> <span class="nav-text">和不同版本的 Hive Metastore 交互</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用-JDBC-连接其他数据库"><span class="nav-number">5.</span> <span class="nav-text">用 JDBC 连接其他数据库</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#疑难解答"><span class="nav-number">6.</span> <span class="nav-text">疑难解答</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#性能调整"><span class="nav-number"></span> <span class="nav-text">性能调整</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#内存缓存"><span class="nav-number">1.</span> <span class="nav-text">内存缓存</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他配置选项"><span class="nav-number">2.</span> <span class="nav-text">其他配置选项</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#分布式-SQL-引擎"><span class="nav-number"></span> <span class="nav-text">分布式 SQL 引擎</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#运行-Thrift-JDBC-ODBC-server"><span class="nav-number">1.</span> <span class="nav-text">运行 Thrift JDBC/ODBC server</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用-Spark-SQL-命令行工具"><span class="nav-number">2.</span> <span class="nav-text">使用 Spark SQL 命令行工具</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#升级指南"><span class="nav-number"></span> <span class="nav-text">升级指南</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-5-升级到-1-6"><span class="nav-number">1.</span> <span class="nav-text">1.5 升级到 1.6</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-4-升级到-1-5"><span class="nav-number">2.</span> <span class="nav-text">1.4 升级到 1.5</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-升级到-1-4"><span class="nav-number">3.</span> <span class="nav-text">1.3 升级到 1.4</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame-数据读写接口"><span class="nav-number">3.0.1.</span> <span class="nav-text">DataFrame 数据读写接口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#DataFrame-groupBy-保留分组字段"><span class="nav-number">3.0.2.</span> <span class="nav-text">DataFrame.groupBy 保留分组字段</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-升级到-1-3"><span class="nav-number">4.</span> <span class="nav-text">1.2 升级到 1.3</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SchemaRDD-重命名为-DataFrame"><span class="nav-number">4.0.1.</span> <span class="nav-text">SchemaRDD 重命名为 DataFrame</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#统一-Java-和-Scala-API"><span class="nav-number">4.0.2.</span> <span class="nav-text">统一 Java 和 Scala API</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#隐式转换隔离，DSL-包移除-–-仅针对-scala"><span class="nav-number">4.0.3.</span> <span class="nav-text">隐式转换隔离，DSL 包移除 – 仅针对 scala</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#移除-org-apache-spark-sql-中-DataType-别名-–-仅针对-scala"><span class="nav-number">4.0.4.</span> <span class="nav-text">移除 org.apache.spark.sql 中 DataType 别名 – 仅针对 scala</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#UDF-注册挪到-sqlContext-udf-中-–-针对-java-和-scala"><span class="nav-number">4.0.5.</span> <span class="nav-text">UDF 注册挪到 sqlContext.udf 中 – 针对 java 和 scala</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Python-DataTypes-不再是单例"><span class="nav-number">4.0.6.</span> <span class="nav-text">Python DataTypes 不再是单例</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Shark-用户迁移指南"><span class="nav-number">5.</span> <span class="nav-text">Shark 用户迁移指南</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#调度"><span class="nav-number">5.1.</span> <span class="nav-text">调度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reducer-个数"><span class="nav-number">5.2.</span> <span class="nav-text">Reducer 个数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#缓存"><span class="nav-number">5.3.</span> <span class="nav-text">缓存</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#兼容-Apache-Hive"><span class="nav-number">6.</span> <span class="nav-text">兼容 Apache Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#部署在已有的-Hive-仓库之上"><span class="nav-number">6.0.1.</span> <span class="nav-text">部署在已有的 Hive 仓库之上</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持的-Hive-功能"><span class="nav-number">6.1.</span> <span class="nav-text">支持的 Hive 功能</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不支持的-Hive-功能"><span class="nav-number">6.2.</span> <span class="nav-text">不支持的 Hive 功能</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#参考"><span class="nav-number"></span> <span class="nav-text">参考</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据类型"><span class="nav-number">1.</span> <span class="nav-text">数据类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NaN-语义"><span class="nav-number">2.</span> <span class="nav-text">NaN 语义</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Dean Wang</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">Site words total count&#58;</span>
    
    <span title="Site words total count">279.2k</span>
  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/bugs/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/bugs/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/bugs/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/bugs/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/bugs/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/bugs/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/bugs/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/bugs/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/bugs/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/bugs/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/bugs/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
